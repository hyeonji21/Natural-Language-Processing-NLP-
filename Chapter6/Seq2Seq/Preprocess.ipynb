{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocess.ipynb","provenance":[],"authorship_tag":"ABX9TyMZg0xoiVrALLLCBVDQb+Cz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gjzJLj9oBrW","executionInfo":{"status":"ok","timestamp":1637718442815,"user_tz":-540,"elapsed":3644,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}},"outputId":"0cae9c42-951d-4e31-d49d-681c35d1c03d"},"source":["!pip install preprocess"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting preprocess\n","  Downloading preprocess-2.0.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from preprocess) (0.16.0)\n","Installing collected packages: preprocess\n","Successfully installed preprocess-2.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"BYvLeVNyh9xr","executionInfo":{"status":"ok","timestamp":1637718688102,"user_tz":-540,"elapsed":840,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["# preprocess 모듈에서 함수를 불러오고, 학습할 데이터 경러와 저장할 단어사전 경로 선언\n","\n","from preprocess import*\n","\n","PATH= '/content/drive/MyDrive/data_in/ChatBotData.csv_short'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZKfVkAeqJEP","executionInfo":{"status":"ok","timestamp":1637719141606,"user_tz":-540,"elapsed":5656,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}},"outputId":"025724d0-73de-4885-df44-f44a174abdcd"},"source":["import os #운영체제의 기능을 사용하기 위해\n","import re #정규표현식을 사용하기 위해\n","import json\n","import numpy as np\n","import pandas as pd #데이터 불러오기\n","from tqdm import tqdm\n","!pip install konlpy\n","from konlpy.tag import Okt  #konlpy : 한글 형태소 활용\n","\n","# 학습에 사용할 데이터를 위한 데이터 처리와 관련하여 몇가지 설정값을 지정한다.\n","FILTERS = \"([~.,!?\\\"':;)(])\"\n","PAD = \"<PAD>\"  #어떤 의미도 없는 패딩 토큰\n","STD = \"<SOS>\"  #시작 토큰을 의미\n","END = \"<END>\"  #종료 토큰을 의미\n","UNK = \"<UNK>\"  #사전에 없는 단어를 의미\n","PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","UNK_INDEX = 3\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(FILTERS)\n","\n","MAX_SEQUENCE = 25\n","\n","# load_data 함수는 데이터를 판다스를 통해 부러오는 함수\n","# 판다스를 통해 데이터를 가져와 데이터프레임 형태로 만든 후 question과 answer를 돌려준다. (inputs, outputs에는 question, answer 존재)\n","def load_data(path):\n","    data_df = pd.read_csv(path, header=0)\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","\n","    return question, answer\n","\n","\n","# 단어 사전을 만들기 위해 -> 데이터를 전처리한 후 단어 리스트로 먼저 만들어야함.\n","                    # -> 이 기능을 수행하는 data_tokenizer 함수를 먼저 정의\n","# 정규표현식 (re)을 사용해 특수 기호를 모두 제거, 공백 문자를 기준으로 단어들을 나눠서 전체 데이터의 모든 단어를 포함하는 단어 리스트 생성\n","def data_tokenizer(data):\n","    words = []\n","    for sentence in data:\n","        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","        for word in sentence.split():\n","            words.append(word)\n","    return [word for word in words if word]"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 7.4 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 51.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"code","metadata":{"id":"PAx2N_zXqnKa","executionInfo":{"status":"ok","timestamp":1637719169335,"user_tz":-540,"elapsed":284,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["# prepro_like_morphlized 함수 : 한글 텍스트를 토크나이징하기 위해 형태소로 분리하는 함수\n","# KoNLPy에서 제공하는 Okt 형태소 분리기를 사용해 형태소 기준으로 텍스트 데이터를 토크나이징.\n","# 형태소로 분류한 데이터를 받아 morphs 함수를 통해 토크나이징된 리스트 객체를 받고, 이를 공백 문자 기준으로 문자열로 재구성해 반환\n","def prepro_like_morphlized(data):\n","    morph_analyzer = Okt()\n","    result_data = list()\n","    for seq in tqdm(data):\n","        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ','')))\n","        result_data.append(morphlized_seq)\n","\n","    return result_data\n","\n","\n","# make_vocabulary 함수\n","def make_vocabulary(vocabulary_list):\n","    # 리스트를 키가 단어이고 값이 인덱스인 딕셔너리를 만든다.\n","    word2idx = {word: idx for idx, word in enumerate(vocabulary_list)}\n","    # 리스트를 키가 인덱스이고 값이 단어인 딕셔너리를 만든다.\n","    idx2word = {idx: word for idx, word in enumerate(vocabulary_list)}\n","    # 두 개의 딕셔너리를 넘겨 준다.\n","    return word2idx, idx2word\n","\n","\n","# 단어 사전을 만드는 함수 정의\n","#  -> 경로에 단어 사전 파일이 없다면 불러와서 사용한다.\n","def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n","    vocabulary_list = []\n","    if not os.path.exists(vocab_path):\n","        if (os.path.exists(path)):\n","            data_df = pd.read_csv(path, encoding='utf-8')\n","            question, answer = list(data_df['Q']), list(data_df['A'])\n","        if tokenize_as_morph:\n","            question = prepro_like_morphlized(question)\n","            answer = prepro_like_morphlized(answer)\n","\n","        data = []\n","        data.extend(question)\n","        data.extend(answer)\n","        words = data_tokenizer(data)\n","        words = list(set(words))\n","        words[:0] = MARKER  # 사전에 정의한 특정 토큰들을 단어 리스트 앞에 추가한 후 마지막으로 이 리스트를 지정한 경로에 저장\n","\n","        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n","            for word in words:\n","                vocabulary_file.write(word + '\\n')\n","\n","    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n","        for line in vocabulary_file:\n","            vocabulary_list.append(line.strip())\n","    word2idx, idx2word = make_vocabulary(vocabulary_list)\n","    # word2idx : 각각 단어에 대한 인덱스 / idx2word : 인덱스에 대한 단어를 가진 딕셔너리 데이터에 해당\n","\n","    return word2idx, idx2word, len(word2idx)  # 단어에 대한 인덱스 / 인덱스에 대한 단어 / 단어의 개수\n","\n","# 인코더 부분 & 디코더 부분 전처리\n","\n","# 인코더에 적용될 입력값을 만드는 전처리 함수\n","# 띄어쓰기를 기준으로 토크나이징 한다.\n","def enc_processing(value, dictionary, tokenize_as_morph=False):   #value:전처리할 데이터 / dictionary:단어 사전\n","    # 인덱스 값들을 가지고 있는 배열. (누적된다.)\n","    sequences_input_index = []\n","    # 하나의 인코딩 되는 문장의 길이를 가지고 있다. (누적된다.)\n","    sequences_length = []\n","\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    # 한줄씩 불어온다.\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence) #정규 표현식을 통해 특수문자 제거 (필터에 들어있는 값들을 \"\"으로 치환)\n","        sequence_index = [] #문장을 스페이스 단위로 자르고 있다.\n","        for word in sequence.split(): #잘려진 단어들이 딕셔너리에 존재하는지 보고 그 값을 가져와 sequence.index에 추가\n","            if dictionary.get(word) is not None:\n","                sequence_index.extend([dictionary[word]])  #단어 사전을 이용해 단어 인덱스로 바꿈.\n","            else:\n","                sequence_index.extend([dictionary[UNK]])  #어떤 단어가 단어 사전에 포함되어있지 않다면 UNK 토큰을 넣는다.\n","\n","        #문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","        #하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","        # max_sequence_length보다 문장 길이가 작다면 빈 부분에 PAD(0)을 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을 sequences_input_index에 넣어 준다.\n","        sequences_input_index.append(sequence_index)\n","\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경\n","    # -> 텐서플로우 dataset에 넣어 주기 위한 사전 작업\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_input_index), sequences_length\n","\n","\n","# 디코더의 입력값을 만드는 함수\n","def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n","    sequences_output_index = [] # 인덱스 값들을 가지고 있는 배열 (누적)\n","    sequences_length = [] # 하나의 디코딩 입력 되는 문장의 길이를 가지고 있다. (누적)\n","\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","\n","        # 하나의 문장을 디코딩할 때 가지고 있기 위한 배열\n","        sequence_index = []\n","        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의 값인 인덱스를 넣어 준다.\n","        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word\n","                                              in sequence.split()]\n","\n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","        # 하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","\n","        # 인덱스화 되어 있는 값을 sequences_output_index에 넣어 준다.\n","        sequences_output_index.append(sequence_index)\n","\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경\n","    # -> 텐서플로우 dataset에 넣어 주기 위한 사전 작업\n","    # 넘파이 배열에 인섹드화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_output_index), sequences_length\n","\n","# 디코더의 타깃값을 만드는 전처리 함수\n","def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n","    # 인덱스 값들을 가지고 있는 배열 (누적)\n","    sequences_target_index = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 문장에서 스페이스 단위별로 단어를 가져와 딕셔너리의 값인 인덱스를 넣어 준다.\n","        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        # 그리고 END 토큰을 넣어 준다.\n","        if len(sequence_index) >= MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n","        else:\n","            sequence_index += [dictionary[END]]\n","\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을 sequences_target_index에 넣어 준다.\n","        sequences_target_index.append(sequence_index)\n","\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경\n","    #  -> 텐서플로우 dataset에 넣어 주기 위한 사전 작업\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_target_index)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzHjNIr4oHin","executionInfo":{"status":"ok","timestamp":1637718463351,"user_tz":-540,"elapsed":277,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["VOCAB_PATH = '/content/drive/MyDrive/data_in/vocabulary.txt'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzjVyuXdoNO7","executionInfo":{"status":"ok","timestamp":1637719173770,"user_tz":-540,"elapsed":277,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["inputs, outputs = load_data(PATH)\n","char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)\n","# load_vocabulary 함수로 단어 사전을 char2idx, idx2char로 만든다.\n","\n","index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)\n","# tokenize_as_morph : 문장 토크나이즈를 띄어쓰기 단위로 할지 형태소 단위로 할지 결정\n","# -> False로 설정하면 띄어쓰기 단위로 토크나이즈"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNzfV07RpgCS","executionInfo":{"status":"ok","timestamp":1637719433475,"user_tz":-540,"elapsed":309,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["# enc_processing & dec_processing, dec_target_processing 함수를 통해 \n","# 모델에 학습할 인덱스 데이터를 구성\n","index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)\n","index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n","index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqDKNvxqrRWi","executionInfo":{"status":"ok","timestamp":1637719599168,"user_tz":-540,"elapsed":269,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["# 단어사전과 특별한 토큰들을 각각 정의해 딕셔너리 객체에 저장\n","data_configs = {}\n","data_configs['char2idx'] = char2idx\n","data_configs['idx2char'] = idx2char\n","data_configs['vocab_size'] = vocab_size\n","data_configs['pad_symbol'] = PAD\n","data_configs['std_symbol'] = STD\n","data_configs['end_symbol'] = END\n","data_configs['unk_symbol'] = UNK"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ72b7-arRZC","executionInfo":{"status":"ok","timestamp":1637719800647,"user_tz":-540,"elapsed":262,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}}},"source":["# 각 인덱스 데이터와 단어사전을 구성한 객체를 numpy와 json형식으로 저장\n","\n","DATA_IN_PATH = '/content/drive/MyDrive/data_in/'\n","TRAIN_INPUTS = 'train_inputs.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIGS = 'data_configs.json'\n","\n","np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n","np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'wb'), index_outputs)\n","np.save(open(DATA_IN_PATH + TRAIN_TARGETS, 'wb'), index_targets)\n","\n","json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"3skDgz-rrRej"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r19tdKhiEPP","executionInfo":{"status":"ok","timestamp":1637716900760,"user_tz":-540,"elapsed":25975,"user":{"displayName":"‍이현지[ 학부재학 / 경제통계학부 국가통계전공 ]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11628902924451157886"}},"outputId":"720dfb45-bbf4-45ce-c94c-0395d33d922f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}